[{
    "title": "On the Economics and Ergonomics of LLMs",
    "date": "",
    "description": "",
    "body": "For all the millions of words that have been devoted to the capabilities of LLMs, the economics and ergonomics of these systems seem to me to be under-discussed. In much the same way that technology changes - from records to CDs to streaming - have been a dominant factor in the evolution of popular music, prosaic matters such as cost structures will have a substantial influence on the development and impact of AI. While it is still early days for LLMs, there are already signs that economic factors are impacting everything from product design and technology choice to adoption and even regulation.\nLLM Economics 101 Let\u0026rsquo;s start with a very high-level view of the fundamental economics of LLM-based AI systems. These systems are extremely large and run on hardware that is very expensive to buy and run. This means that there are significant CAPEX and OPEX associated with providing a first-class LLM offering, such as OpenAI or Anthropic. Importantly, there is a quantifiable per-unit cost incurred by LLM invocations; more precisely, a per-token cost. This cost structure creates an incentive structure unfamiliar to the modern software world. For many applications, the software is not valuable enough to any one company to justify all the cost of its creation and maintenance. Software companies solve this by building once and selling to a massive global market. The fixed cost of software is high, but the marginal cost is zero (or close to it). With LLMs, the fixed cost is high, and the marginal cost is also high. This means that companies need to attract large numbers of consumers, but also need to either limit their consumption or charge for it.\nConversational, Embedded, and Agentic AI Currently, users consume LLMs through three principal mechanisms: conversational AI, embedded AI, and agentic AI. Conversational AI is the familiar ChatGPT experience - the user experiences a discussion with an LLM that has access to your discussion history, any documents you upload, and often basic tools such as web browsing and function execution. The economics of this interaction are simple: the user pays a fixed monthly fee, and they consume the service as much as they like, usually up to some rate limit of messages per hour. The provider manages the cost. The ergonomics of this interaction are a mixed bag; while well-suited to simple questions and answers, actually getting this AI to do anything for you usually involves some form of copying and pasting between the chat application and the application where you want to do work. This is unergonomic and very limiting in terms of capability.\nEmbedded AI is the AI that you experience as part of another application that has integrated LLM functionality. An example might be Notion’s built-in AI writer, or Microsoft’s Copilot inside Word or Excel. For providers of this software, the LLM is an enhancement to an existing value proposition. Just like any feature, by integrating it into their software, the developer is hoping to either gain an advantage in the market or - as is increasingly the case with AI - maintain feature parity with competitors (“table stakes”). The ergonomics of embedded AI are very good for the application you are engaging with, but non-existent for all other applications. This means that the value of the AI to the customer is a strict subset of the value provided by the application itself. The economics of embedded AI come in one of two flavours - either the functionality is charged for in a modular way like GitHub Copilot, or it is provided to the user as straight consumer surplus like Notion AI. In either form, the economics of the AI system rest on the role of the functionality securing revenue for the primary product.\nAgentic AI is an emerging type of AI that strings multiple LLM invocations together, with both hardcoded and LLM-supervised logic to coordinate their activity. These AI agents work together to achieve higher-level, more abstract, or more wide-ranging goals, using many LLMs in narrow and specific ways to achieve specific outcomes within the system. These AI systems are usually capable of using a wide variety of tools, and are granted significant agency by the user to pursue end goals by whatever means they have available. Agentic AI is a nascent and currently niche use of LLM technology, but it comes the closest to achieving the classical conception of an AI assistant. The ergonomics of agentic AI are fantastic when the systems work as intended. At their current stage of development, the systems are quite “hit and miss”, but they can achieve impressive results. The economics of these systems are proportionally expensive. Each task the agents perform potentially involves multiple LLM invocations, with each invocation creating a marginal cost. Coupled with the probability that getting the desired result from an agentic system may require a trial-and-error approach, the costs of achieving an outcome can become unexpectedly large.\nHaving Your Cake\u0026hellip; One way to avoid the per-token economic costs of using a third-party LLM provider such as OpenAI is to host your own LLM. Thanks to open-source efforts from companies such as Meta, there are world-class LLMs freely available for commercial use. Hosting your own first-class LLM is a daunting task for many companies, not to mention an expensive one. While the per-use cost of the system can be quite low, there needs to be a high level of confidence in the business that LLM usage will be high for a sustained period of time. While this is not dissimilar from the familiar Total Cost of Ownership (TCO) calculation that enterprise IT departments do as a matter of course, few enterprise technologies have seen as much rapid growth and rapid change as LLMs, making a standard TCO calculation extremely difficult.\n\u0026hellip; and Eating it Too Enter LLM routers. A router is a system that sits between a client application and an LLM, and routes requests to the appropriate LLM. This allows a single client application to use multiple LLMs, either to achieve better performance, or to achieve better economics. Routers can be used to route requests to different LLMs based on the type of request, the size of the request, the performance characteristics of the service, latency, or any other factor that the router’s administrator can think of. Importantly from our perspective, routers can also be used to route requests to different LLMs based on the cost of the request, allowing companies to prioritise their spend according to their own needs.\nThe approach of using an LLM router which routes between a smaller, easier-to-self-host LLM and a first-class, third-party service allows companies to have their cake and eat it too. Requests that require top performance can be routed to the third-party service, while requests that are less performance-critical can be routed to the self-hosted LLM. In particular, the multitude of requests initiated by agentic AI systems can be initially handled by the self-hosted LLM with its fixed cost structure, only resorting to third-party services when the smaller LLM is not up to the task, or once the task has been fully refined through trial and error. This is a highly ergonomic approach for users, with predictable and manageable economic characteristics.\nCaveat Promptor The economics and ergonomics of LLM-based AI systems are an important and under-discussed element of the current technological revolution. The expensive nature of first-class LLMs makes enterprise adoption at scale - particularly of agentic AI - a somewhat risky business. Fortunately, as we have seen time and time again, where technology creates a problem, the solution is usually more technology. LLM routers, open source, self-hosting, and other software-centric approaches to managing these systems are emerging rapidly. As usual, those companies that lean into these trends will reap the greatest benefits.\n",
    "ref": "/blog/the-economics-and-ergonomics-of-ai/"
  },{
    "title": "The Decomposition of Large Language Models",
    "date": "",
    "description": "",
    "body": "While many AI systems can be very complex, Large Language Models (LLMs) seem to be refreshingly simple to understand. With their ability to generate human-like text, applications like ChatGPT have captured the popular imagination. However, as compelling an experience as chatbots are, the tradeoffs that they make can lead us to misunderstand LLMs as a whole. In this post, we will explore the idea that the functions of LLMs can be thought of as discrete capabilities, and how different composition of these functions leads to a much wider variety of user experiences.\nThe Discrete Capabilities of Large Language Models We can think of LLMs as being composed of several functions that work together to produce the final output. These functions include, but are not limited to, input interpretation, embedding and embedding search, synthesis, and generation.\nInput Interpretation: the LLM takes the user\u0026rsquo;s query and interprets it in a way that it can understand and respond to. This function is crucial for effective communication between the user and the model. Embedding and Embedding Search: the LLM takes the interpreted input and transforms it into a numerical representation, known as an embedding. This embedding can then be searched to find similar embeddings in the model\u0026rsquo;s knowledge base, allowing it to draw on relevant information when generating a response. Synthesis: the LLM takes a set of documents and combines them into a coherent narrative. This involves connecting disparate pieces of information and presenting them in a way that makes sense to the user. Generation: the LLM generates a natural-language response. This is the function that most users interact with and see the results of. The Implicit Composition of ChatGPT ChatGPT, one of the most well-known LLM-powered applications, composes these functions implicitly. It takes a user\u0026rsquo;s query and generates a response in a zero-shot manner, drawing on all of the knowledge it has gathered during its training phase. This approach has its advantages, such as the ability to generate diverse and creative responses. However, it also has its drawbacks, the most notable of which is the tendency to hallucinate, or generate information that is not accurate or grounded in reality. This has led many to believe that such hallucination is an inherent feature of LLMs, but this is not necessarily the case.\nDifferent Composition Equals Different Experience By composing the functions of an LLM differently, we can achieve a different tradeoff of functionality and, consequently, a different user experience. For instance, we can use the LLM\u0026rsquo;s embedding function to create a searchable index. This index can return results using embedding search, which can then be combined with the LLM\u0026rsquo;s interpretive capability to transform imprecise user queries into precise lookups.\nThe results of this search can then be synthesised from a disconnected group of documents into a coherent narrative, based on verifiable sources. This approach can help mitigate the issue of hallucination, as the responses are grounded in verified information, leading to a more accurate and reliable user experience.\nThis is just one example among multitudes of the recomposition of LLM functions. The key to understanding the true capabilities of LLMs is to cease viewing them as autonomous agents possessing intelligence in the human sense. Instead, they are better viewed as mathematical constructs that can be manipulated by computer programs. These functions, when combined in the manner of \u0026ldquo;traditional\u0026rdquo; programming, can pave the way for more nuanced and precise user experiences. This approach allows us to harness the capabilities of LLMs in a more controlled and directed manner, mitigating some of the limitations associated with their use as chatbots.\nThe Potential of LLMs Lies Beyond Chatbots By viewing LLMs through this lens, we can also uncover a broader range of applications than what might be apparent from an experience with chatbots alone. The discrete functions of LLMs can be leveraged in numerous ways, each offering unique possibilities for innovation and advancement. While chatbots are a compelling user experience for demonstrating the power of the technology, they are both limiting and misleading as paradigms of what LLMs can be used to achieve.\n",
    "ref": "/blog/decomposition-of-llms/"
  },{
    "title": "The Large Language Model Iceberg",
    "date": "",
    "description": "",
    "body": "As the capabilities of Artificial Intelligence (AI) continue to evolve, the term \u0026ldquo;chatbot\u0026rdquo; has become synonymous with a new era of digital interactions. What was once a seemingly futuristic concept is now an integral part of our everyday lives. However, it is the Machine Learning models underlying them - Large Language Models (LLMs) - that are the true powers in these waters.\nChatbots are just the tip of the iceberg Chatbots, like ChatGPT, are incredibly useful tools that have revolutionised our digital interactions. Their ability to understand and process natural language allows them to seamlessly engage in conversations with humans, providing an unprecedented level of customer service and user experience. But as impressive as chatbots may be, they represent only a fraction of what LLMs are truly capable of achieving.\nThe impressive performance of chatbots is just the tip of the iceberg when it comes to the capabilities of LLMs. These models possess the power to go far beyond basic conversational interactions and can be utilised in various fields such as content generation, summarization, and translation. The true potential of LLMs, however, lies in their ability to bridge the gap between the complex world of human expression and the precise world of databases.\nA context-aware translation layer The true power of LLMs lies in their ability to create a context-aware translation layer between the rich but messy world of human expression and the precise world of databases. This translation layer can interpret and process unstructured data, allowing for more accurate and efficient retrieval of information. In just a few short weeks, there has been a Cambrian explosion of frameworks and libraries to allow LLMs to connect to data sources both public and private, using their ability to synthesise data to combine disparate sources into coherent insights. By understanding the nuances of human language, LLMs can transform vast amounts of complex and disorganised data into valuable applications.\nA new wave of business innovation The remarkable capabilities of LLMs have already given rise to a new generation of companies built around the idea of using these models to turn locked-away data into valuable applications. These businesses leverage the power of LLMs to transform unstructured data into actionable insights, opening up new avenues for innovation and growth. As more companies begin to recognise the potential of LLMs, we can expect to see a surge in novel applications that will shape the future of numerous industries.\nChatbots have provided us with a glimpse into the extraordinary capabilities of LLMs. Yet, we must be cognizant of the fact that chatbots represent merely the initial foray into the expansive potential of these models. As the business landscape starts to recognise and embrace the full extent of LLMs\u0026rsquo; capabilities, we are poised to witness a burgeoning wave of innovative applications that will reshape the way we engage with technology and unleash the true potential of AI. In this transformative era, those who are capable of appreciating and harnessing the vast possibilities offered by LLMs stand to reap unparalleled rewards, both intellectually and commercially.\nTitanic shifts Intriguingly, the metaphor of the \u0026ldquo;iceberg\u0026rdquo; proves to be particularly fitting in more ways than one. It was, after all, an iceberg that brought about the untimely demise of the \u0026ldquo;unsinkable\u0026rdquo; Titanic, demonstrating that even the most seemingly invulnerable entities can succumb to unforeseen forces. Similarly, the transformative capabilities of LLMs may well prove to be the proverbial iceberg for today\u0026rsquo;s seemingly indomitable tech giants. As these titans navigate the ever-shifting seas of technological innovation, it is worth considering that the true potential of LLMs, still largely hidden beneath the surface, may usher in a new era of digital disruption, redefining the competitive landscape and challenging the supremacy of even the most established players. In this tempestuous ocean of technological advancements, adaptability and foresight will be the compass guiding organizations to success, lest they, too, find themselves adrift amidst the unforgiving currents of change.\n",
    "ref": "/blog/the-large-language-model-iceberg/"
  },{
    "title": "Three Categories of AI",
    "date": "",
    "description": "",
    "body": "The term \u0026ldquo;AI\u0026rdquo; has at least three entirely different meanings in the modern business context, and any particular business leader usually has only one of these meanings in mind when they use the term. Not only is it common for participants in a discussion to have different interpretations of the term \u0026ldquo;AI\u0026rdquo;, but most will be unaware that this terminological confusion exists, leaving leaders in both public and private sector organisations poorly equipped to discuss one of the most important technologies of our time.\nThree Types of AI I contend that there are three distinguishable things that people mean when they use the term \u0026ldquo;AI\u0026rdquo;:\nAI as Advanced Analytics, AI as Applied Machine Learning, and AI as Future Magic. Each of these is a valid way to view AI in the modern business context, but each leads to a radically different business strategy and technological implementation.\nAI as Advanced Analytics Category One AI is about providing interpretable information to decision-makers using increasingly-advanced, data-driven methods. This may include very sophisticated machine learning algorithms, extensive software development, and novel research, but as a business operation it occupies the same role as business intelligence. An unkind but not inaccurate aphorism would be \u0026ldquo;extreme dashboarding\u0026rdquo;.\nThis view of AI is neither aspirational nor transformative, but it is concrete and has the convenient feature of keeping the executive at the centre of the system. Advanced Analytics not only retains the role of the decision-maker as the lynchpin of the operations of the firm, but enhances and entrenches it. This speaks very clearly and directly to the incentives that individual leaders face within competitive organisations, and is therefore extremely attractive to them.\nAI as Future Magic Leaving aside discussion of Category Two for narrative effect, Category Three AI is speaks to the deep-seated need leaders often have for their organisation to be \u0026ldquo;more\u0026rdquo;; more innovative, more profitable, more effective, more agile, more exciting. They are the captain of a vessel that they want to turn like a speedboat but handles like a container ship. Convinced of both the power of modern technology and the wealth of data that they must surely have, the Future Magic of AI allows them to imagine a world in which they can be a visionary leader of a dynamic firm without bothering with the much more difficult work of figuring out how such a dramatic transformation could possibly be made.\nIt is much too easy to parody such views, but with Future Magic AI there is a complicating wrinkle that goes some way to absolving people who believe in it: many of their ideas about this Future Magic are inspired by real, present-day technology that they have seen with their own eyes. Discovering the structure of proteins with AlphaFold, generating movie scripts with GPT-3, and inventing photorealistic 3-D scenes with neural radiance fields are all real possibilities that look like Future Magic.\nWhat\u0026rsquo;s more, this Future Magic seems to be created by people who look very much like people they know. Sure, Google DeepMind might employ people at the absolute top of their game, but they know quite a few very smart people with PhDs in String Theory and Algebraic Geometry who didn\u0026rsquo;t want (or couldn\u0026rsquo;t find) a postdoc position and now work in their Data Science team. They can read the latest research and create novel algorithms too. So if AI can do all this Future Magic, and the people who make the Future Magic are pretty similar to their people, surely they can have some Future Magic too.\nUnfortunately, these quite obvious ingredients of transformative, novel AI technologies are necessary but not sufficient for the real application of Future Magic AI. These advanced technologies are not only developed by organisations with large budgets and an incredible depth of technical talent, but other systems that support the creation and deployment of new, highly complex technologies; networks of systems, processes, and cultural norms that enable these technologies to have an effect on the real world operations of the organisation. Businesses struggling with low-complexity operations in low-productivity environments are very far away from having the structures necessary to make this technology effective. To put it bluntly, one cannot put a Ferrari engine in a 1994 Honda Civic and expect to win a Grand Prix.\nMoreover, many firms do not have the organisational or operational complexity to make sophisticated technologies valuable. Part of AI\u0026rsquo;s particular value proposition is its ability to manage complexity. New Zealand, where I come from, has a particularly acute case of this problem in that not only do many firms lack the kind of complexity that makes Future Magic AI invaluable, but the entire economy lacks that sophistication.\nThus, the executive who looks to Future Magic AI to solve their organisational woes is not being foolish or unreasonable; this technology does exist, it just isn\u0026rsquo;t useful or practical for their firm.\nAI as Applied Machine Learning We now come to Category Two AI. AI in the sense of Applied Machine Learning is the meat in the sandwich; it\u0026rsquo;s the advanced technology that almost any organisation can leverage to immediate and often dramatic effect. This technology is accessible, affordable, and in common use. The only problem with it is that we don\u0026rsquo;t call it AI. Instead, we call it Natural Language Processing or Computer Vision. Precisely because this technology is real, useful, and within reach, it\u0026rsquo;s not considered to be AI.\nIn my conversations with businesses and government agencies, I am continually trying to find better language to talk about AI in the sense of Applied Machine Learning. I have found two main roadblocks to making these conversations successful. The first is the disparate nature of the terminology; indeed, from the perspective of a business conversation it is Applied Machine Learning\u0026rsquo;s defining feature. This means that even if a fruitful conversation can be started about Computer Vision, for example, there is no logical connection in the mind of the executive between the topic we\u0026rsquo;re currently discussing and the need they have to classify their archive of audio recordings. The lack of a useful overarching term inhibits the connection of business problems to technological solutions.\nThe second difficulty is that Applied Machine Learning is neither as easily understandable and non-threatening as Advanced Analytics nor as inspiring and carefree as Future Magic. This makes it attractive primarily to people within the organisation whose motivation is to achieve real outcomes. These pragmatic individuals are, unfortunately, often quite distant from those holding the purse strings, which tends to reduce their ability to create organisational change and real business value.\nWhat\u0026rsquo;s in a name? The words that we use to describe the things we do shape, enhance, and restrict how we collectively think about our work, and therefore what it is possible to achieve in a business. The disparate interpretations that people have of the term AI, compounded by the nescience of the existence of these differences, inhibits the use of real, practical technologies and limits their potential to transform organisations. Business cultures that find more precise ways to talk about AI will reap outsized rewards and, perhaps as importantly, make for less frustrating conversations.\n",
    "ref": "/blog/three-categories-of-ai/"
  },{
    "title": "If it's not mysterious, it's not AI",
    "date": "",
    "description": "",
    "body": "Businesses are deeply confused about what AI is, and they have every right to be. Differences in terminology have plagued the Data-AI-Software landscape for many years, as anyone who has ever looked for a job as a \u0026ldquo;Data Scientist\u0026rdquo; will attest. Attempts to demystify the topic either conjure a series of analogies that are then strained to breaking point, or try to delve into the actual technology behind the hype, leaving everyone more confused than they were before.\nI will attempt something different: removing the mystery by embracing the mystery.\nA Mystery Wrapped In An Enigma The \u0026ldquo;AI Effect\u0026rdquo; is the idea that as soon as we can do something, it\u0026rsquo;s no longer AI. As Rodney Brooks puts it,\nEvery time we figure out a piece of it, it stops being magical; we say, \u0026lsquo;Oh, that\u0026rsquo;s just a computation.\u0026rsquo;\nI always thought that this was a true and insightful point, but its focus was on people who are involved in, or comment on, the development of AI. The quote refers to people looking at a result that they might previously have claimed was only achievable through intelligence, understanding the technical implementation, and reclassifying what they believe AI to be.\nRecently, I\u0026rsquo;ve come to understand that the point applies equally, if not more so, to the business understanding of AI: not as a technology but as a product. To someone in 1700, multiplying large numbers was something only a highly educated human could do, yet today the humble pocket calculator vastly outperforms the smartest human in this field. Nobody looks at a pocket calculator and thinks that it\u0026rsquo;s an Artificial Intelligence. AI is always out of reach, and therefore anything within reach cannot be AI.\nThe modern resurgence of business interest in AI has predictably prompted a new wave of AI confusion, only these days the technologies that are not mysterious enough to be AI are even more sophisticated and intelligent. Among the products of what we might call \u0026ldquo;Applied Machine Learning\u0026rdquo; are the plethora of Computer Vision, Natural Language Processing, and Speech-to-Text technologies that businesses are familiar and comfortable with. Even if they are not making extensive use of these technologies themselves, they know others who are, and can understand pretty much what they do. While they may be impressed by them, they are not mystified by them.\nAI, on the other hand, is a mysterious technology that will radically transform the business - if only they can figure out what it means. The packaging of AI technologies into easily consumable products does not serve to demystify AI, because as soon as they are so packaged, they are no longer AI: they\u0026rsquo;re Computer Vision or Named Entity Recognition. AI will always be mysterious to the enterprise because it is defined as that which is still mysterious.\nSo what do we call it, then? There\u0026rsquo;s a rapidly-aging witticism that AI is a logistic regression done in Silicon Valley. In addition to being untrue and unfair, this adage also epitomises the misplaced tactic of demystifying AI by pulling back the curtain to reveal the machinery. Businesspeople find the machinery no more enlightening than they did the hype. Though the technologist understands that neural radiance fields, image registration, and 3D scene reconstruction all use related mathematics and computational techniques, that explanation does nothing to help orient the thinking of the business.\nThe solution might be to embrace the fact that in the enterprise, AI exists only at the board level: the realm of vision and positioning. At any level below that, be it strategy or operations, only the specific types of applied machine learning exist, along with the business needs they address. Computer vision, natural language processing, and document summarisation are products and solutions the business can get a mental grip on. They aren\u0026rsquo;t mysterious.\nAnd if it\u0026rsquo;s not mysterious, then it\u0026rsquo;s not AI.\n",
    "ref": "/blog/its-not-ai/"
  },{
    "title": "The Memo",
    "date": "",
    "description": "",
    "body": "In 2002, Amazon\u0026rsquo;s Jeff Bezos issued a memo that has entered tech industry canon. The memo, known as the \u0026ldquo;API Mandate\u0026rdquo;, is generally perceived as being a statement about technology at Amazon, and is therefore widely admired by technologists and wholly ignored by executives. This is unfortunate, because it\u0026rsquo;s no exaggeration to say that the API Mandate completely transformed Amazon as a business and laid the foundation for its success. Better still, unlike many things that global technology titans do, it is something that can be replicated and put to use by almost any business.\nIn this post, we\u0026rsquo;ll talk about the memo, and how it created the systems and incentives for radical organisational transformation.\nThe Memo The memo itself is reported1 to have read,\nAll teams will henceforth expose their data and functionality through service interfaces. Teams must communicate with each other through these interfaces. There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team’s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. It doesn’t matter what technology they use. HTTP, Corba, Pubsub, custom protocols — doesn’t matter. All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. Anyone who doesn’t do this will be fired. Thank you; have a nice day! To understand what this means, let\u0026rsquo;s take an example. Suppose you are hired by Amazon to start selling home improvement products to the Australian market. There are a bunch of things you\u0026rsquo;re going to do to get started, like getting some new product categories set up in the database, putting listings on the Amazon website, figuring out which products to stock and how many, and getting Amazon warehouses to store those products2.\nAt Amazon, each of these atomic functions is a series of API calls. An API - an Application Programming Interface - is a piece of software that allows other software to talk to it and access its functionality. APIs are found everywhere in all kinds of technologies, and this is what Bezos is referring to when he writes \u0026ldquo;service interfaces\u0026rdquo;.\nBusiness Programming Interfaces There may be several hundred of these atomic functions necessary to achieve a business outcome. In most organisations, each of them is accomplished by holding multiple meetings with myriad stakeholders, achieving alignment between teams, getting others to prioritise your work in their schedules, and other high-touch, human-centric processes. Anybody who has seen this in action at an organisation of any size will know that this can result in delays of months or years - if anything happens at all.\nIn our example, the API Mandate requires the team responsible for listings on the website to expose the things that their team does as an API, rather than requiring interpersonal communication. You want a new listing? Call the API - there\u0026rsquo;s your listing. No interminable meetings, misalignments, delays, or self-inflicted organisational chaos3. These APIs are not so much APIs as BPIs \u0026ndash; Business Programming Interfaces.\nThe importance of this difference cannot be overstated. This is the secret to Amazon\u0026rsquo;s ability to rapidly enter any market that it sets its eye on. It might seem improbable, but Amazon\u0026rsquo;s incredible growth can largely be attributed to the API Mandate4. To paraphrase Benedict Evans, it has turned Amazon into \u0026ldquo;a machine for making more Amazon\u0026rdquo;.\nSystems Eat Culture The critical mechanisms for the proper operation of the API mandate are that it is systematic and that it aligns incentives.\nIt is common for attempted organisational transformations to fail because enacting the transformation requires many hundreds or thousands of people to pull in the same direction, often against their own self-interest. This often devolves into a \u0026ldquo;war for hearts and minds\u0026rdquo; or a \u0026ldquo;culture shift\u0026rdquo;, ignoring the reasons why those hearts and minds and cultures aren\u0026rsquo;t already pulling in that direction.\nFor transformations to take hold, they need to be based on a system of mechanisms that incentivise every individual or unit to independently reinforce the system. The systematic implications of the API Mandate are now clear, but the effectiveness of the system relies on incentive mechanisms to ensure participation.\nAmazon\u0026rsquo;s system of openly available metrics incentivises every team to provide their function in a way that maximises the ability of other teams to make use of that function. In many ways, this mimics the function of the market itself - the best way to make money in a competitive market is to be as useful as possible to your customers.\nTogether, the system and its incentives create a natural force pushing in the direction of organisational transformation. There is no ongoing need to cajole or threaten recalcitrant units into taking part, but rather a continual process of evolution and improvement along the vectors created by the system. Culture is an effect, not a cause.\nThree Lessons The API Mandate reads as if it were written by an Engineering leader rather than the CEO. It doesn\u0026rsquo;t mention anything about business goals, strategy, targets, or even products or customers. All of the things that business people care about are noticeably absent, and yet when the impact of the mandate is understood, it\u0026rsquo;s probably the most important single memo in the history of business. There are three lessons to be drawn from this.\nThe first lesson is that any corporate of sufficient complexity could implement the API Mandate itself and achieve at least a good approximation of Amazon\u0026rsquo;s flexibility and scalability. Whether every corporate has leadership skilled enough to know how to exploit those advantages is, of course, another matter.\nThe second lesson is that organisational change is achieved through systems and incentives, rather than culture. Technology is a prime catalyst for this type of change, as its structured and orderly nature lends itself to systematic application.\nThe third lesson is that technology is not only much more important in business than executives would like to think, but that realising the potential of technology means fundamentally transforming the core operations of the business to exploit the characteristics of the technology. If business strategy is not developed by people with a strong, big-picture understanding of technology, then that strategy is almost certainly doomed to underperform.\nWhere Are All The Amazons? One significant reason why there are so few businesses like Amazon might be that very few executives understand or even like technology enough to be able to effectively exploit it for competitive advantage. It\u0026rsquo;s common to see even highly sophisticated companies ignoring the incentives that structure their business and ignoring the systematic approaches to business transformation that technology enables.\nThe good news is that companies like Amazon have blazed clear trails for the rest of us to follow. With a clear understanding of what the API Mandate really means, real transformation is within any organisation\u0026rsquo;s grasp.\nAll you need is a strongly-worded memo.\nLike an Arthurian legend, the memo has entered the realm of myth. Whether or not it existed in this exact form, the API Mandate was and is real at Amazon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThese details are complete conjecture, but the point stands.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAt least in theory. I\u0026rsquo;m sure some of this still happens in practice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAt the very least as an emblem of Amazon\u0026rsquo;s systematic institutional approach to scaling its organisation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "ref": "/blog/the-memo/"
  },{
    "title": "The Business Implications of Combinatorial Optimisation by Deep Learning",
    "date": "",
    "description": "",
    "body": "Recently, Deep Learning has revolutionised a multitude of technical fields, most notably Computer Vision and Natural Language Processing. The implications for business are enormous, with the ramifications of these change still filtering through the mostly technology-shy world of business. There are signs that Combinatorial Optimisation might be the next technical field to undergo Revolution-By-Neural-Network. If this comes to fruition, then the impact on business will be seismic.\nCombinatorial Optimisation Combinatorial Optimisation (CO) sounds very dry and technical when defined; It is the process of finding an optimal object from a set of objects. The Travelling Salesman Problem, a classic and well-studied problem in Computer Science, falls into this category, as do problems as diverse as vehicle routing and workforce scheduling.\nCO problems are so ubiquitous in industry that it\u0026rsquo;s fair to say that every business of at least a medium size is impacted by them (whether they are aware of it or not), as well as a good many smaller ones. For some problems, such as those found in supply chain management, entire industries exist merely to assist with small parts of that overall challenge.\nThere are, of course, many methods currently employed to deal with problems arising in CO. Any number of software companies and consultants offer solutions in supply chain management, for example. So why is it that solving these issues with Deep Learning would be such a revolutionary event?\nAlexNet Moments To understand this, we need to look at the recent history of Computer Vision (CV) and Natural Language Processing (NLP). As recently as a few years ago, solutions to both CV and NLP problems possessed the following characteristics:\nHighly specialised algorithms. A \u0026ldquo;big bag of tricks\u0026rdquo; approach. Scale is a problem. A great deal of subject-matter expertise. A general lack of success in solving real-world problems. Today, both NLP and CV problems are addressed with Deep Learning, with the following set of characteristics:\nVery general algorithms. Consistent approach. Scale is an advantage. A great deal of Deep Learning expertise. Huge success in solving real-world problems. Former specialists with years of inherited wisdom about how to approach CV and NLP were not particularly happy with this sea change, but the results speak for themselves: High quality, scalable NLP and CV are available to any company that wants them.\nThe turning point for CV was the 2012 release of AlexNet - a Convolutional Neural Network (CNN) that won the ImageNet Large Scale Visual Recognition Challenge. After AlexNet, the only methods to win image recognition contests were based primarily or solely on CNNs. Similarly, in 2017, the publication of the Transformer architecture for NLP problems in a paper entitled \u0026ldquo;Attention Is All You Need\u0026rdquo; created a paradigm shift in NLP model performance. The events that unlock a new, Deep-Learning-based approach to a field are called \u0026ldquo;AlexNet moments\u0026rdquo;.\nThe Bitter Lesson The particular Neural Network architectures of CNNs and Transformers per se are not very important. As I have previously written, what is important is that both derive their power from three critical characteristics:\nThe method is general. The model architecture allows for scalable computation. The quality of the model scales well with compute. These \u0026ldquo;Bitter Lesson Conditions\u0026rdquo; derive their name from Richard Sutton\u0026rsquo;s seminal essay The Bitter Lesson, which begins:\nThe biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.\nThe observation is simple, but the implications are profound; when we find a method that satisfies these three conditions, we revolutionise a field. At the time of writing, Automatic Speech Recognition (ASR) is going through an AlexNet moment because we have discovered methods that satisfy the Bitter Lesson Conditions.\nThe Next Frontier Returning to the realm of Combinatorial Optimisation, we can see that solutions to CO problems presently have the following characteristics:\nHighly specialised algorithms. A \u0026ldquo;big bag of tricks\u0026rdquo; approach. Scale is a problem. A great deal of subject-matter expertise. A general lack of success in solving real-world problems. These are the exact conditions that pertained in CV and NLP problems prior to their respective AlexNet moments. What CO currently lacks is an algorithm that satisfies the Bitter Lesson Conditions.\nExcitingly - or worryingly, depending on your perspective - there are signs that this may be changing. Researchers are actively working towards scalable Deep Learning approaches to CO problems. For now, the problems are toy problems and classical algorithms are still the go-to solution for difficult cases, but it is hard to believe that the same forces that have rapidly transformed stubborn domains such as CV, NLP, and ASR will not eventually crack the CO nut.\nBack to Business The upshot for business is that while CV and NLP may be critical technologies, they are small beer compared to the world of CO. Combinatorial Optimisation problems are so core to the operations of so many businesses, that the explosion of solutions and software unleashed by an AlexNet moment would be Cambrian in scale, and the impact on the business landscape incalculable.\nGiven how fundamental such processes are to many businesses, and given also the scale of the commercial advantage that early adopters will obtain, I hold out some mild hope that this may be the technology that finally breaks the \u0026ldquo;IT is overhead\u0026rdquo; attitude of many large enterprises. With enough determination from management, recent advances in CV and NLP can be kept at arm\u0026rsquo;s length from the core operations of the business, and it is apprently still possible for an executive in 2021 to truly believe that technology is best bought and stuck in the corner, far away from where the action truly happens. The business-critical nature of CO problems will, with any luck, force executives to confront the reality that ongoing effective use of technology is now a real strategic imperative, not a cost to be born.\nNeural Networks in the Boardroom It\u0026rsquo;s a long bow to draw from experimental methods that may be just over the horizon to my personal vendetta against the exclusion of technologists from key strategic positions in traditional companies, but the ingredients for an AlexNet moment are all there - not just for Combinatorial Optimisation, but for business itself.\n",
    "ref": "/blog/business-implications-of-combinatorial-optimisation/"
  },{
    "title": "The Bitter Lesson Conditions",
    "date": "",
    "description": "A modest proposal for revolutionising everything.",
    "body": "The hard truth of AI is that methods that exploit deep, hard-won human knowledge about a particular domain are outperformed by methods that cleverly exploit the increasing power of computation.\nI have a modest proposal to distill this fundamental truth into a set of conditions which, when met, will revolutionise a given field.\nLineage I do not claim this insight about the fundamental mechanism of progress in AI as original; I have been heavily influenced by Richard Sutton\u0026rsquo;s powerful essay The Bitter Lesson.\nAs he puts it,\nThe biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.\nSutton is not alone in this observation. The somewhat eccentric polymath Gwern has made similar remarks, prompted by the success of the GPT-3 language model.\nThe Bitter Lesson Conditions The Computer Vision revolution of 2012, the Natural Language revolution of 2017, and the Speech Recognition revolution of 2020 are all concrete examples of this phenomenon, and I think that at this point it is safe to say that the mechanism behind the Bitter Lesson is the driving force in modern Deep-Learning-based AI.\nIf we take the Bitter Lesson seriously, these revolutions, far from being surprising, are almost inevitable. However, it\u0026rsquo;s not clear to me that this observation gives us any specific predictive power. Given a particular field, under what conditions can we expect AI to revolutionise it?\nI have my own modest proposal for a framework to think about answering such questions, namely, that there are three conditions to be met for a method to revolutionise a problem space:\nThe method is general. The model architecture allows for scalable computation. The quality of the model scales well with compute. For want of a more elegant term, I have dubbed these the \u0026ldquo;Bitter Lesson Conditions\u0026rdquo;.\nGenerality and Scalability The first condition is that the method should be general. In this context, this means that the method should reliably address an entire problem space, rather than one or more subproblems. This definition begs the question somewhat, but a moment\u0026rsquo;s thought about any specific issue will yield a suitable boundary for a problem space. Take Natural Language Processing (NLP), for example. While there are many blurry edges such as image captioning, few would disagree that for a method to be \u0026ldquo;general\u0026rdquo;, it should at least address core concerns such as Named Entity Recognition, Sentiment Analysis, and Topic Extraction. The more subproblems a method can address, the more general it is, and the greater its impact is.\nThe second and third conditions both concern scalability, but use the term in two quite different senses. The second condition is the most commonly discussed meaning of model scalability, namely, how well the architecture lends itself to massively parallel and/or distributed computation. This is a topic of general importance, but for the Bitter Lesson it is central. If the algorithm cannot actually make use of the available computational power, then the mechanism breaks down.\nThe meaning of scalability in the third condition is less commonly used, but is also quite obviously necessary for the mechanism to function. The algorithm must of course scale well in the sense that adding more computing power improves the performance of the model, however there is a more subtle and important effect of this type of scalability: beyond quality as measured by simple metrics, scaling the algorithm should lead to a difference in the kind of behaviour the system exhibits.\nIn the case of GPT-3, the poster child for this type of scalability, increasing the scale of the computation led the algorithm to display entirely new, emergent properties. As Gwern put it,\nThese benefits were not merely learning more facts \u0026amp; text than GPT-2, but qualitatively distinct \u0026amp; even more surprising in showing meta-learning: while GPT-2 learned how to do common natural language tasks like text summarization, GPT-3 instead learned how to follow directions and learn new tasks from a few examples.\nThis more subtle sense of the term scalability is, in my view, the truly revolutionary aspect of the three conditions. The reason for not extracting it as a distinct fourth condition is that it remains to be seen whether emergent behaviour is strictly necessary for revolution, as opposed to simple improvement with scale.\nPowers of Prediction I find the Bitter Lesson Conditions to be a useful framework for my own thinking about a problem space. It helps give structure to my own questions about whether, say, Geometric Deep Learning is about to experience a paradigm shift in capability, or whether we are still in the realm of incremental improvement.\nWhat I don\u0026rsquo;t yet know is whether the conditions are highly predictive. If I\u0026rsquo;m very brave, I may venture some predictions in the future, and test these conditions against the reality. For now, I\u0026rsquo;m content with having a framework to think about coming advances in AI.\n",
    "ref": "/blog/the-bitter-lesson-conditions/"
  },{
    "title": "Odds and Ends #4",
    "date": "",
    "description": "Kops, podcasts, and Milvus.",
    "body": " I just discovered kops, apparently an excellent way to deploy and manage Kubernetes clusters on the cloud. Since managing a k8s cluster seems to be a full-time job at the best of times, I\u0026rsquo;m interested to see if this really does make it much easier. I have a very mild case of podcast addiction, and am always looking for something great to add to my collection of subscriptions. Nikita Voloboev has published a very interesting list of podcasts. I have tried a few of the ones I didn\u0026rsquo;t already know, and they are excellent. Speaking of podcasts, I highly recommend The Dropout, the result of a three-year investigation by ABC News into the fraud perpetrated by Elizabeth Holmes and her company Theranos. Milvus is a welcome new addition to the space of vector search. Vector similarity search is extremely useful any time you are dealing with high-dimensional embeddings, and it has been my observation that thinking about embeddings often leads to new and interesting opportunities quite orthogonal to the original use case for which the embeddings were produced. ",
    "ref": "/blog/odds-and-ends-4/"
  },{
    "title": "Odds and Ends #3",
    "date": "",
    "description": "Karothek, Jupyter on K8s, and more.",
    "body": " My former colleagues at Blue Yonder (now part of JDA) have introduced Kartothek, software for managing tables stored as parquet files. A great set of documentation on going zero to JupyterHub with Kubernetes. A related piece from Jim Crist on installing JupyterHub on an existing Hadoop cluster. An interesting paper interoducing PATE from a couple of years ago that had passed me by. PATE stands for Private Aggregation of Teacher Ensembles, and is a method for doing semi-supervised transfer learning from private data. ",
    "ref": "/blog/odds-and-ends-3/"
  },{
    "title": "Odds and Ends #2",
    "date": "",
    "description": "Information theory, an AI Playbook, and Traces.",
    "body": " Naftali Tishby on the Information Theory of Deep Learning (embedded below). I\u0026rsquo;m very enthusiastic about this kind of work, and have resolved to find and read more of it. a16z\u0026rsquo;s AI Playbook may be a couple of years old now, but it\u0026rsquo;s still an important read. Traces is a Python library for unevenly-spaced time series analysis. I haven\u0026rsquo;t had a problem to really try this out on, but the website looks slick 👌🏻. ",
    "ref": "/blog/odds-and-ends-2/"
  },{
    "title": "Odds and Ends #1",
    "date": "",
    "description": "Spektral, RPC Frameworks, and Python packaging",
    "body": " Spektral is a framework for relational representation learning, built in Python and based on the Keras API. The field of Geometric Deep Learning is starting to get some traction. In particular, doing Deep Learning on graphs presents some interesting possibilities. RPC Frameworks: gRPC vs Thrift vs RPyC for python. The state of Python Packaging. Packing in Python is both improving dramatically and harder than it should be. ",
    "ref": "/blog/odds-and-ends-1/"
  },{
    "title": "Keybase: Keys for everyone! Part II",
    "date": "",
    "description": "",
    "body": "In a previous post, we discussed Keybase, a clever company that is solving a lot of the classic problems that users have had with PGP: managing keys, verifying identities, trusting third parties, and user experience.\nI also mentioned that Keybase insures iteself against an attack on its servers by writing the root of its Merkle tree into the blockchain. In this post, we\u0026rsquo;ll explore what this means, and go through the process of verifying this ourselves step-by-step.\nBefore we rush ahead to the verification, let\u0026rsquo;s take a moment to understand what a Merkle root is, and why Keybase is using them.\nSuppose that we have some discrete blocks of data, and that we would like to be able to verify the integrity of these data. Concretely, let\u0026rsquo;s suppose that every time I do something on Keybase, such as verify myself on some social media account or revoke a device key, Keybase creates a JSON file which describes this activity, and stores this on its servers. Another user of Keybase can then go to Keybase\u0026rsquo;s servers and collect this JSON blob from them, in order to verify my public activity. To be even more precise, the data that the other user really wants is my signature chain; a cryptographically signed chain that records my activity.\nHowever, there\u0026rsquo;s a problem. Servers are notoriously hackable, and if someone were to compromise Keybase\u0026rsquo;s server, I could be downloading a JSON file that is telling me any old lies. Now, a compromised server\u0026rsquo;s ability to lie is dramatically restricted by the fact that any user can verify the signature chain themselves, checking external social media sites for the claimed proofs. However, a compromised server would be able to show different versions of the server state to different users.\nThis is where the Merkle tree enters the equation. A Merkle tree takes each of the pieces of data we would like to verify, the \u0026ldquo;leaves\u0026rdquo; of the tree, and hashes them. Several of these hashes are then collected together according to a defined algorithm, and the concatenation of them is then itself hashed. These concatenations are then also grouped and hashed, and so on. This heirarchical grouping forms a tree, with each concatenated hash forming a node of the tree. Eventually, we end up with one final, concatenated hash. This hash is the root of the tree. Once in possession of the root hash and the algorithm used to group and concatenate the hashes, we can verify the state of every single leaf node at the point in time that they were hashed, giving us a picture of a consistent server state. All that is needed now is for us to be able to trust the root hash of the Merkle tree, which is achieved by signing the root hash with a private key.\nThe utility of a Merkle tree for keybase now becomes obvious. This system is, however, potentially vulnerable to a sophisticated \u0026ldquo;forking\u0026rdquo; attack. One example of such an attack might be that I revoke a compromised key, and publish this change to the comprimised Keybase server. The attacker chooses not to present an updated version of the Merkle tree to another user, and exploits his lack of knowledge of my published revokation.\nKeybase solves this problem by writing the root of the Merkle tree to the Bitcoin blockchain by sending bitcoins from a known address to another. Instead of using the bitcoins, Keybase is using 160 bits of the receiving address to encode the root hash of the Merkle tree. To be a little more precise, Keybase generates a Merkle tree, then signs the root hash of the tree with its private key. It then generates a 160-bit hash of this signature, and sends Bitcoins from their address to the address represented by this hash. Once we know the address receiving the Bitcoins, we can compare the root hash of the Merkle tree presented to us by Keybase\u0026rsquo;s servers to that stored in the blockchain, rendering the forking attack useless.\nLet\u0026rsquo;s go through this process step by step, pausing where appropriate to make sure that we really understand what\u0026rsquo;s going on. We\u0026rsquo;ll use Python 3.5 to do this, partly because it is my language of choice for most tasks, but mostly because the original keybase security documentation uses Python, albeit version 2.7. Let me be clear right from the outset that the code is taken almost entirely from the keybase security documentation, and credit for it should be attributed to the Keybase team. My contribution is limited to making the code work with Python 3.5, and a few cosmetic edits for legibility.\nWe start by importing the packages that we\u0026rsquo;ll need.\nimport requests, datetime, json, re, gnupg from base64 import b64decode from pycoin.encoding import bitcoin_address_to_hash160_sec, hash160 from binascii import hexlify from hashlib import sha512, sha256 Recall that what we need to find is the 160-bit hash of the signed root of the Keybase Merkle tree, which is stored in the blockchain as the receiving address for the latest transaction from the sending address 1HUCBSJeHnkhzrVKVjaVmWg2QtZS1mdfaz. Without downloading the multi-gigabyte Bitcoin blockchain in its entirity, the easiest way to find this address is to query blockchain.info using the requests library. Requests is a beautifully written python library, that is often cited as the gold standard for clean, pythonic APIs. I recommend learning to use its basic functionality even for simple cases such as ours.\nWe use a simple GET request to query the blockchain.info API.\nfrom_addr = \u0026#34;1HUCBSJeHnkhzrVKVjaVmWg2QtZS1mdfaz\u0026#34; uri = \u0026#34;https://blockchain.info/address/%s?format=json\u0026#34; % (from_addr) r = requests.get(uri) to_addr = r.json()[\u0026#39;txs\u0026#39;][0][\u0026#39;out\u0026#39;][0][\u0026#39;addr\u0026#39;] to_addr_hash = hexlify(bitcoin_address_to_hash160_sec(to_addr)).decode(\u0026#39;utf-8\u0026#39;) print(to_addr_hash) 1eeea91d88d3578e3e718fa97da8ec79d7227304 Notice that we have decoded the string from UTF-8. We will see this decoding and encoding of strings to and from UTF-8 appears several times as we progress. The difference from the keybase.io example arises because of the different way that Python 3.5 handles strings, and the encoding expectations of the different libraries that we are using.\nGiven that our code varies from that presented on keybase.io, it would be reasonable to ask whether this encoding and decoding in some way invalidates the verification that we are undertaking. These processes merely alter the representation of the string, not its content. It is not the string object as it appears in computer memory that we are verifying, but the evaluated content of the string. Theoretically, we could even perform the verification process by hand, separating the content of the string entirely from its representation in computer memory. We are therefore on farily safe ground.\nThe variable to_addr_hash now contains a hexadecimal hash, which corresponds to a root block on the Keybase servers. We can query the Keybase API to find that block.\nkb = \u0026#34;https://keybase.io/_/api/1.0\u0026#34; uri = \u0026#34;%s/merkle/root.json?hash160=%s\u0026#34; % (kb, to_addr_hash) r = requests.get(uri) root_desc = json.loads(r.text) The root_desc variable now holds a JSON file full of information about the root block. What we need is the signature of the hash of the root block, which is contained in the sig field. Noting again the string encoding and decoding, we use a regular expression to extract the signature itself from the surrounding ASCII armouring, and verify with an assert statement that the hexadecimal hash of the signature matches the hexadecimal hash of the receiving bitcoin address that we found in the first step.\nsig = b64decode(re.compile(r\u0026#34;\\n\\n((\\S|\\n)*?)\\n=\u0026#34;).search(root_desc[\u0026#39;sig\u0026#39;]).group(1).encode(\u0026#39;utf-8\u0026#39;, \u0026#39;ignore\u0026#39;)) if to_addr_hash == hexlify(hash160(sig)).decode(\u0026#39;utf-8\u0026#39;, \u0026#39;ignore\u0026#39;): print(\u0026#34;We have the correct block.\u0026#34;) else: print(\u0026#34;Oops, wrong block!\u0026#34;) We have the correct block. Recall that this hash is not the root hash itself. Rather, it is the 160-bit hash of the Keybase PGP signature of the root hash. This makes sense, as we would like to be sure not just that the Merkle tree is intact, but that Keybase has verified it as being correct.\nKeybase\u0026rsquo;s Merkle key has the fingerprint 03E146CDAF8136680AD566912A32340CEC8C9492, and is imported into my GPG keyring. The gnupg python package allows us to inspect this signature and verify it.\ngpg = gnupg.GPG() verified = gpg.verify(sig) print(\u0026#39;Signed by {0},\\n with public key fingerprint {1},\\n on {2}.\\n\u0026#39;.format(verified.username, verified.pubkey_fingerprint, datetime.datetime.utcfromtimestamp( float(verified.sig_timestamp)) .strftime(\u0026#39;%d/%m/%Y\u0026#39;))) if verified.valid: print(\u0026#39;Signature is valid.\u0026#39;) else: print(\u0026#39;Signature invalid!\u0026#39;) Signed by Keybase.io Merkle Signing (v1) \u0026lt;merkle@keybase.io\u0026gt;, with public key fingerprint 03E146CDAF8136680AD566912A32340CEC8C9492, on 20/02/2017. Signature is valid. Now we know that not only is the Merkle tree is intact, but that Keybase has verified it as being correct, and when they did so. The signature object contains the root hash of the root block of the Merkle tree. We can use another regular expression to find it.\nroot_hash = json.loads(re.compile(r\u0026#34;({[\\x00-\\x7f]*})\u0026#34;).search(sig.decode(\u0026#39;utf-8\u0026#39;, \u0026#39;ignore\u0026#39;)).group(1))[\u0026#39;body\u0026#39;][\u0026#39;root\u0026#39;] print(\u0026#39;Root block hash: %s\u0026#39; % root_hash) Root block hash: 55a8731fdb7141f0e8441d01f0431624de9d275952e7dcb19bb35088a0e5cf90312c67cb0fc876a5a888b61fa545c1261bc2732ac2d759e6f9488b30ff902f28 Now that we have the hash of the root block, let\u0026rsquo;s check that the Keybase server wasn\u0026rsquo;t lying to us about the contents of the block.\nuri = \u0026#34;%s/merkle/block.json?hash=%s\u0026#34; % (kb, root_hash) value_string = requests.get(uri).json()[\u0026#39;value_string\u0026#39;] computed_hash = hexlify(sha512(value_string.encode(\u0026#39;utf-8\u0026#39;)).digest()).decode(\u0026#39;utf-8\u0026#39;) if computed_hash == root_hash: print(\u0026#39;Root hash and computed hash match.\u0026#39;) else: print(\u0026#39;Root hash: %s,\\n\u0026#39;%root_hash, \u0026#39;Computed hash: %s\u0026#39;%computed_hash) Root hash and computed hash match. Let\u0026rsquo;s pause and think about what we\u0026rsquo;ve discovered. We know that Keybase created a Merkle tree, which encodes the exact status of every user\u0026rsquo;s signature chain at the time that the tree was written. The root hash of this tree was then signed by Keybase, and then this signature was hashed and written into the blockchain.\nTo verify this, we walked backwards through the process. We went to the blockchain and found the latest hash, and asked Keybase for the signed Merkle tree corresponding to this hash. We verified that the hash of this signature corresponded to the hash that we found in the blockchain, and then verified that the signature came from Keybase. Having done that, we checked that the hash we were given is correct by computing the hash ourselves, and comparing the two.\nWe can now descend the Merkle tree to find the last thing I signed into my sigchain. You can use any user, as this is public information (indeed, that\u0026rsquo;s the point!), so try your own.\nusername = \u0026#34;chrislaing\u0026#34; uri = \u0026#34;%s/user/lookup.json?username=%s\u0026#34; % (kb, username) r = requests.get(uri) uid = r.json()[\u0026#39;them\u0026#39;][\u0026#39;id\u0026#39;] Let\u0026rsquo;s define a couple of helper functions to make descending the tree easier. The trees are written in such a way that the JSON string returned by the API has as keys the first few characters of the hash. The method I use in traverse_tree is actually quite ugly, as it simply checks each potential key length until it finds a match. The data is so small, however, that I didn\u0026rsquo;t see the need to write a prettier way of doing it - if you\u0026rsquo;d like to do this, then please leave a comment, and I\u0026rsquo;ll update the post.\ndef traverse_tree(start_hash): uri = \u0026#34;%s/merkle/block.json?hash=%s\u0026#34; % (kb, start_hash) value_string = requests.get(uri).json()[\u0026#39;value_string\u0026#39;] computed_hash = hexlify(sha512(value_string.encode(\u0026#39;utf-8\u0026#39;)).digest()).decode(\u0026#39;utf-8\u0026#39;) assert(computed_hash == start_hash) tab = json.loads(value_string)[\u0026#39;tab\u0026#39;] for i in range(len(uid)+1): if uid[:i] in tab.keys(): print(\u0026#39;key length %d of %d\u0026#39;%(i, len(uid))) nxt = tab[uid[:i]] if i == len(uid): return nxt[1][1] else: return nxt def verify_link_hash(link_hash): uri = \u0026#34;%s/sig/get.json?uid=%s\u0026#34; % (kb, uid) payload = requests.get(uri).json()[\u0026#39;sigs\u0026#39;][-1][\u0026#39;payload_json\u0026#39;] computed_hash = hexlify(sha256(payload.encode(\u0026#39;utf-8\u0026#39;)).digest()).decode(\u0026#39;utf-8\u0026#39;) return (computed_hash == link_hash.encode(\u0026#39;utf-8\u0026#39;)) def get_computed_hash(uid): uri = \u0026#34;%s/sig/get.json?uid=%s\u0026#34; % (kb, uid) payload = requests.get(uri).json()[\u0026#39;sigs\u0026#39;][-1][\u0026#39;payload_json\u0026#39;] computed_hash = hexlify(sha256(payload.encode(\u0026#39;utf-8\u0026#39;)).digest()).decode(\u0026#39;utf-8\u0026#39;) return computed_hash def get_payload(uid): uri = \u0026#34;%s/sig/get.json?uid=%s\u0026#34; % (kb, uid) payload = requests.get(uri).json()[\u0026#39;sigs\u0026#39;][-1][\u0026#39;payload_json\u0026#39;] return payload The idea is very simple. Using my user ID, we query the Keybase api for the last action that I signed into my sigchain. To verify that this exists in the version of the Merkle tree that we verified above, we walk down the Merkle tree, progressively querying the Keybase API for each new node, and then comparing the hash of what Keybase tells us to the hash we expect. In this way, we verify all the contents of the Merkle tree from the root, down the branches that lead to my leaf, until we arrive at my leaf block itself.\nlink_hash = root_hash computed_hash = get_computed_hash(uid) while link_hash != computed_hash: link_hash = traverse_tree(link_hash) if link_hash == computed_hash: print(\u0026#39;Leaf node found in the Merkle tree.\u0026#39;) else: print(\u0026#39;Leaf node is not part of the Merkle tree!\u0026#39;) key length 1 of 32 key length 2 of 32 key length 3 of 32 key length 4 of 32 key length 32 of 32 Leaf node found in the Merkle tree. Now that we know that the server state that Keybase is reporting to us is correct, consistent, and signed by Keybase, we can inspect the payload we got from the API to see what I actually did.\njson.loads(get_payload(uid)) { \u0026#39;body\u0026#39;:{ \u0026#39;key\u0026#39;:{ \u0026#39;eldest_kid\u0026#39;:\u0026#39;0101c0eab8dc42e80f0a754adc7d5504249ca3dc2d6c848407e6e148729c800a25010a\u0026#39;, \u0026#39;host\u0026#39;:\u0026#39;keybase.io\u0026#39;, \u0026#39;kid\u0026#39;:\u0026#39;0120d945e916db5015ff9e8d5605e822e3808c90602a043fe08aa43b91449a5de6b40a\u0026#39;, \u0026#39;uid\u0026#39;:\u0026#39;4ac28b770f9cd1199981621a42eac000\u0026#39;, \u0026#39;username\u0026#39;:\u0026#39;chrislaing\u0026#39; }, \u0026#39;service\u0026#39;:{ \u0026#39;name\u0026#39;:\u0026#39;reddit\u0026#39;, \u0026#39;username\u0026#39;:\u0026#39;laing_c\u0026#39; }, \u0026#39;type\u0026#39;:\u0026#39;web_service_binding\u0026#39;, \u0026#39;version\u0026#39;:1 }, \u0026#39;client\u0026#39;:{ \u0026#39;name\u0026#39;:\u0026#39;keybase.io go client\u0026#39;, \u0026#39;version\u0026#39;:\u0026#39;1.0.18\u0026#39; }, \u0026#39;ctime\u0026#39;:1487579548, \u0026#39;expire_in\u0026#39;:504576000, \u0026#39;merkle_root\u0026#39;:{ \u0026#39;ctime\u0026#39;:1487579413, \u0026#39;hash\u0026#39;:\u0026#39;f37e1e249b16110bb5ad4fdaba7cc044ee86ce6aae1efb1c51eedf6ca9d955a6cd400e19dc9211e68ad9cf74a4b2082aa767c831ff625417a936e82b6d6857fd\u0026#39;, \u0026#39;seqno\u0026#39;:907120 }, \u0026#39;prev\u0026#39;:\u0026#39;bf84e1e3dd41765dd105ee814bc4b81dcba6a3988f5c12896b56ab2d430ecb9b\u0026#39;, \u0026#39;seqno\u0026#39;:45, \u0026#39;tag\u0026#39;:\u0026#39;signature\u0026#39; } In this case, I proved my reddit identity, laing_c. The JSON blob also contains lots of useful details about the proof, such as the time that I signed it, the details of the client used to sign the proof, and so on.\nIn case you\u0026rsquo;re curious, here\u0026rsquo;s the proof.\nNow we know how to verify that Keybase hasn\u0026rsquo;t been taken over by a malicious third party, who is seeking to lie to us - either actively or by omission - about the state of the Keybase server.\nI really applaud the team at Keybase for going to this extent. It\u0026rsquo;s fantastic that they have managed to combine a very accessible day-to-day user experience with a completely open and verifiable trust system. I haven\u0026rsquo;t even touched on some of the other magic they\u0026rsquo;ve been working, such as the Keybase file system, but I might come to that in a future post.\nI hope that you enjoyed this little walk through the Keybase blockchain verification system, and that you found it somewhat helpful in understanding how Keybase works.\n",
    "ref": "/blog/keybase-keys-for-everyone-part-ii/"
  },{
    "title": "Keybase: Keys for everyone! Part I",
    "date": "",
    "description": "",
    "body": " Encryption is a topic that has fascinated me for years, less because of its mathematical basis than its applications, both existing and potential. By far my favourite project in this realm is Keybase.\nKeybase cleverly solves several major problems with PGP-based encryption technology by taking the attitude that online, your identity is functionally the sum of your online profiles. With Keybase, you can automatically verify that @cjlaing on twitter has a particular key, and can encrypt messages or files for him without having to ascertain and validate his key through some other medium. The client software is open source, and works seamlessly with both PGP and NaCl keys.\nOne of the truly excellent features of Keybase\u0026rsquo;s design is its trust model; although their website and file hosting service offers a centralised interface, that interface is entirely orthogonal to the operation of the service. When you want to id someone and verify their key (me, for example), the Keybase client goes to each of their listed web identities, finds the proof, and verifies it, without needing to trust the keybase.io website at all. Every time a user verifies someone\u0026rsquo;s identity by tracking them, this proof is written into the blockchain. The entire chain can then be independently verified, without requiring anyone to trust Keybase itself.\nThe Keybase filesystem is an extremely clever extension of the core Keybase concept. What I find most interesting is the idea that, in principle, data can be securely shared with someone who hasn\u0026rsquo;t yet signed up for Keybase, whom you know purely through one of their online identities. When they join Keybase and prove their ownership of that identity, your client will verify the proof and encrypt the file for them.\nKeybase is still in an invitation-only testing phase, however they already have over 100,000 users as of the time of writing. The project is bringing a lot of great new ideas together with solid, tried, and tested cryptography, and I\u0026rsquo;m excited to see where the project goes next.\nIn Part II of this post, we\u0026rsquo;ll take a look at how to verify Keybase proofs yourself, and traverse a user\u0026rsquo;s cryptographic chain using Python.\n",
    "ref": "/blog/keybase-keys-for-everyone-part-i/"
  },{
    "title": "A New Hope",
    "date": "",
    "description": "",
    "body": "Many years ago, I had a blog. Being perfectly honest, it wasn\u0026rsquo;t a very good blog; it consisted mainly of rants of one kind or another, inane musings, and the occasional photo from my holidays. On the modern web, social media is the default depository for said rants, musings, and photos, and so I had no need of a dedicated space on my website to share them.\nHowever, it occurred to me recently that I now share quite a bit of material relating to my work and academic interests; blog posts, interesting new projects, snippets of code, or more general articles related to science and technology. I distribute these nuggets of enlightenment primarily through closed channels - slack, hangouts, messaging apps - with the sole exception of twitter, depriving my many adoring fans of interesting material, and me of feedback.\nSo today, my blog is now open again for business, this time with (hopefully) fewer boring political rants, and more interesting links. I hope that you will find it to be of some interest.\n",
    "ref": "/blog/a-new-hope/"
  },{
    "title": "About Me",
    "date": "",
    "description": "",
    "body": " About me I\u0026rsquo;m Christopher. I live in Wellington, New Zealand, and work at a specialist data firm where I am the Director of Artificial Intelligence and Data Science.\nI received my PhD in Applied Mathematics, as well as my undergraduate degrees in Law and Mathematics, from the University of Otago.\nMy doctoral thesis, entitled Numerical Construction of Static Fluid Interfaces with the Embedding Formalism, is an attempt to reformulate the Young-Laplace equation for static fluid interfaces in the language of differential geometry, and to devise a numerical implementation for the system by Finite Elements.\nMy thesis is available for download from the University of Otago OUR Archive.\n",
    "ref": "/about/"
  }]
